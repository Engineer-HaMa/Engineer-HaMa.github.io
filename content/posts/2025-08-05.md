+++
title = "네이버클라우드 권세중 이사 강연 메모"
date = "2025-08-05"

+++

# 네이버클라우드 권세중 이사 강연 메모

TCO(Total Cost Ownerheip) - 총 소유 비용

## 네이버클라우드 기술 스택과 경향화/최적화 기술
어떻게하면 저전력의 하드웨워를 만들고, 이를 평가하고, 활용할 수 있을까?

## AI transformation (Humanity;s Last Exam)
대학 수준의 문제도 잘 풀더라~
벤치마크 자체가 의미가 없는 영역이 있는데 이것에 너무 빨리 도달함
우리가 지능의 영역이라고 믿었던 부분이 사실 기계적이고 반복적인 행동이다(재무에서의 엑셀 처리, 코딩 어시스턴트...)

코로나 때 개발자 수요가 급증했다가, 코로나 떄 급락함

우리는 어떻게 배우는가?
- 교수님/선배에게 깨져가며? 배운다
- 이 과정이 생략되고 학생이 이러한 과정 없이 GPT를 돌리기 시작함
- 구르지 않고있지 않은가?

이제 굴려줄 곳이 없다 -> 굴려주는 곳은 대학원밖에 없다

## System 1 Thinking -> System 2 Thinking
Reasoning을 하는 건 언제인가? -> LLM에게 혼잣말을 시키니 Thinking이 일어나더라 (CoT)
이렇게 늘려두었더닝 hallicination이 줄고 심사숙고한 대답이 나오더라
ex. n자리 덧셈
이러한 능력을 발현시키는 방법을 알아냄

## pre-time training -> Test-time scaling
학습을 위한 데이터가 한계에 도달했고, 다른 기법을 찾기 시작
Reasoning 없이 Reasoning 모델과 비슷한 수준에 도달시키려면 9배정도의 compute가 필요

## 정체된 모델 사이즈에도 추론 비용이 늘어나고있다
모델 사이즈가 정체되기 시작, MoE가 정답인지 고민
하지만 토큰수를 늘려서 추론 비용이 늘어나고 있다. context 길이가 길어지는 것.
모델 사이즈는 비슷한데 input 사이즈가 커지고 있는건가? * 그렇가면 사용자가 체감하는 input 길이는 그대로인가?
알보이는 토큰이 10배 이상이고 계속 늘어날 것이다.
input token 가격은 그대로이고 output token 가격은 늘어나고 있다.

## 문제는 새로운 GPU가 이를 해결해 주지 않는다는 것
반도체가 다음 세대가 나오면서 기존의 문제를 해결했는데, 지금의 AI 반도체는 가성비가 조금씩 나빠지고 있다.
에너지도 문제이다. 랙 하나에 들어가는 전기가 엄청 늘어나고 있음.
네이버클라우드 데이터센터가 세종에 있는것은 옆에 변전소가 있어서. 이게 다 비용이다
한국은 단일 핵에 120kW을 제공할 수 없다. 랙 하나에 4U밖에 못들어가니 문제가 생긴다.
서비스를 가지고 규모의경제를 구현하려면 면적이 부족하다.
LLM은 레이턴서가 중요하지 않음. 가장 싼 지역에서 돌려고 네트워크 레이턴시보다 추론 시간이 훨씬 길다.

## Blackwell
블랙웰은 chiplet구조이고,

## 추론 비용의 평가
TCO = 장비가격 + 상면비 + 전기료 <- 전부 3-5년치
TPM(Tokens per Minute) == Decoding Throughput
토근당 원가 = TCO / TPM
AI Factory: 토큰을 입력으로 받고 출력으로 내놓는 공장
최적화를 통해 생성토큰당 단가를 낮추는 것이 중요하다

가성비로 봤을 때 HBM이 제일 싼 상황

---

##
